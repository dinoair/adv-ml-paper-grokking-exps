---
data:
  train: "dataset/lcquad/tmcd_split/english_train_split_coef_0.1.json"
  dev: "dataset/lcquad/tmcd_split/english_dev_split_coef_0.1.json"
  test: "dataset/lcquad/tmcd_split/english_test_split_coef_0.1.json"
  dataset_vocab: "dataset/lcquad/query_vocab.json"

model:
  used_model: "t5"
  vanilla:
    tokenizer: "cointegrated/rubert-tiny"
    model: "cointegrated/rubert-tiny"
    enable_attention: True
    epochs_num: 120
    bert_finetune_rate: 0.00006
    learning_rate: 0.0005
    bert_warmup_init_finetuning_learning_rate: 0.00003
    warm_up_init_learning_rate: 0.0005
    warmup_steps: 4000
    batch_size: 32
    n_last_layers2train: 1
    use_pretrained_embeddings: False
    pretrained_embeddings_path: "experiments/pretrained_lm_sparql_embs.pt"
    embeddings_size: 300
  t5:
    tokenizer: "t5-small"
    model: "t5-small"
    epochs_num: 100
    learning_rate: 0.001
    warmup_steps: 1000
    batch_size: 32


test_one_batch: False
run_name: "en_tmcd_erm_t5_v3"

# for testing
model_type: 'erm'
save_model_path: "experiments"
inference_model_name: "en_tmcd_erm_t5_v3/epoch_99_gm_0.46_em_0.21_t5.pt"